<html xmlns="http://www.w3.org/1999/xhtml">
<head>


  <title>VideoPose Dataset </title>
  <meta http-equiv='Content-Style-Type' content='text/css' />
  <link rel='stylesheet' href='/pubfarm/skins/bitterbensapp/bitter.css' type='text/css' />
  <!--HTMLHeader--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  .editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none; }

--></style>  <meta name='robots' content='index,follow' />

</head>
<body>
<?php include_once("googleanalytics.php") ?>

<div class="container">
    <div class="main">

        <div class="body">
        <!--PageText-->
<div id='wikitext'>
<div class="img"><img width='100%' src='http://www.vision.grasp.upenn.edu/video/images/vpose2qual.png' alt='' title='' /></div>
<div class='vspace'></div><h2>Download</h2>
<a href=https://drive.google.com/folderview?id=0B4K3PZp8xXDJSW5ZY1Y3U2pDS0E&usp=sharing>VideoPose 2.0, released June 27, 2011.</a>
  <p class='vspace'>Annotation is stored in MATLAB format in <code>clips.mat</code>.  See <code>view_clips.m</code> for usage.  The train/dev/test split we used in our paper, <a class='wikilink' href='https://drive.google.com/file/d/0B4K3PZp8xXDJS0RXWDJGMUQxVlE/view?usp=sharing'>Parsing Human Motion with Stretchable Models</a>, is included in struct <code>partitions</code> in <code>clips.mat</code>.
</p>
<div class='vspace'></div><h2>Alternate versions</h2>
<p>IF for some reason you want the original, uncropped video frames (for e.g., better background subtraction), OR the cropped images, but every frame (we only used every other frame in the above), you access them from the link above.
They are 
</p>
<div class='vspace'></div>
VideoPose2-fullframes.zip	<br>
VideoPose2-every-frame.zip<br>
VideoPose2-fullframe-every-frame.zip	<br>
<p class='vspace'>You  can use <code>clips(i).examples(j).imgfile</code> to index the files as before.  Also, <code>clips(i).examples(j).cropbox</code> is the box used to crop down to obtain the images in the VideoPose2/images directory.
</p>
<div class='vspace'></div><h2>Description</h2>
<p>The dataset consists of 44 short clips, 2-3 seconds in length, with a total of 1,286 frames. We use 26 clips for training, recycle 1 training clip for a development set, and use 18 for testing. The dataset fixes global scale and translation of the person, as is typically assumed in order to avoid confounding detection errors with pose estimation errors.
</p>
<p class='vspace'>We developed this dataset for the challenging task of tracking upper and lower arms in conjunction with our CVPR2011 paper, Parsing Human Motion with Stretchable Models.  It consists of video clips taken from the TV shows Friends and Lost. We chose to focus on these parts because the remaining upper body parts (head, torso) can be localized with near perfect accuracy with current methods given a detection window, whereas lower arm localization performance of state-of-the- arts methods is still quite poor. Furthermore, most things we are interested in knowing about humans involve the handsâ€”e.g., action recognition, gesture identification and object manipulation.
</p>
<p class='vspace'>Clips in the dataset were hand-selected (before developing our system) to highlight natural settings where state-of-the-art methods fail:
</p>
<div class='vspace'></div><ul><li><strong>A highly varied (yet realistic) range of poses:</strong>
</li></ul><div  style='text-align: center;' class="img"><img width='400'  src='http://www.vision.grasp.upenn.edu/video/images/vpose2scatterplot.png' alt='' title='' /></div>
<div class='vspace'></div><ul><li><strong>Rapid gesticulation:</strong>
</li></ul><div  style='text-align: center;' class="img"><img width='250'  src='http://www.vision.grasp.upenn.edu/video/images/vpose2optflow.png' alt='' title='' /></div>
<div class='vspace'></div><ul><li><strong>A significant portion of frames (30%) with foreshortened lower arms:</strong>
</li></ul><div  style='text-align: center;' class="img"><img width='250'  src='http://www.vision.grasp.upenn.edu/video/images/vpose2larmhist.png' alt='' title='' /></div>
<div class='vspace'></div><h2>License</h2>
<p>MIT
</p>
</div>

        </div>
    </div>
    <div class="clearer"><span /></div>
<!--PageFooterFmt-->
    <div class="footer">
      <a href="http://www.grasp.upenn.edu/index.html">GRASP Laboratory</a><br />
      <a href="http://www.seas.upenn.edu">School of Engineering and Applied Science</a><br />
      <a href="http://www.upenn.edu">University of Pennsylvania</a><br />
      Site maintained by <a href="mailto:benjamin.sapp@gmail.com">benjamin.sapp@gmail.com</a><br />
      </span>
    </div>
</div>
<!--HTMLFooter-->
</body>
</html>
